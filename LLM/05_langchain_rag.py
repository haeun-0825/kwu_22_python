# RAG(검색 증강 생성)
#   - Retrieval Augmentaed Generation
#   - LLM 기본적으로 학습하는데 천문학적인 금액 소모
#       ㄴ LLM을 현재 학습했을 시점까지의 정보만 알고 있음
#       ㄴ 예를 들어, 2025년 05월 31일 학습을 한 모델의 경우
#          6월 1일부터의 정보는 없음!
#       문제점 1: 최신 데이터 반영 X
#       문제점 2: 할루시네이션(환각) → LLM 거짓말쟁이
#       → RAG

# ** RAG **
#   - Human(Query) → LLM(GPT) → Answer
#   - Human(Query) + 검색결과 → LLM(GPT) → Answer

# 예를 들어: 삼성전자 모바일 사업부 갤럭시 QnA 챗봇을 개발 임무
#   ㄴ 기존에 LLM은 갤럭시 스마트폰에 대한 자세한 정보 X
#   ㄴ 사용자가 갤럭시에 대한 질문을 하면 그냥 보편적인 답변만 생성
#   ㄴ 갤럭시 스마트폰에 대한 정보가 담긴 PDF파일 생성
#   ㄴ PDF파일로부터 사용자의 질문과 유사한 정보를 Retrieval(검색)
#   ㄴ 검색 된 정보를 사용자 질문과 함께 LLM에게 전달

# RAG 아키텍처
#   1. 지식 베이스
#        ㄴ 자료(PDF, PPT, Excel) → 불러오기(loader()) → Chunk 단위로 분할 
#           → Text Embedding → Vector DB에 저장

# * Text Embedding
#   - 단어 또는 문장을 1차원 숫자값의 나열로 변환 → 벡터
#   - [1, 5, 3, 2, 7] → 벡터
#   - 임베딩시 고려할 사항은 청크를 몇개의 숫자로 표현?
#       ㄴ 1521개의 숫자로 표현

# PDF → Chunk 1 → 벡터(5, 2, 12, ...) n=1521
#     → Chunk 2 → 벡터(1, 52, 3, ...) n=1521
#     → Chunk 3 → 벡터(99, 3, 4, ...) n=1521